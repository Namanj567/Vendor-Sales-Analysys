{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c7897e2",
   "metadata": {},
   "source": [
    "Vendor Sales Analysys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cd9e8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is the very first project where we performing each and every steps which a full stack data analyst need\\nwhile loading the data to creating aggregated table to make a complete etl pipeline and then using python find \\ninside from it while performing eda on it and then from those insights create a powerbi dashboard and then\\nmake a final report of the project the problem statement was already mentioned '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''this is the very first project where we performing each and every steps which a full stack data analyst need\n",
    "while loading the data to creating aggregated table to make a complete etl pipeline and then using python find \n",
    "inside from it while performing eda on it and then from those insights create a powerbi dashboard and then\n",
    "make a final report of the project the problem statement was already mentioned '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6814dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so first we were doing the sql part that can be also done using any type of sql/postgresql and here we were doing this \n",
    "# by just using sqlite and creating connection in that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "086f3234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # to perform etl task\n",
    "import os # get data from the operating system\n",
    "from sqlalchemy import create_engine #sqlalchemy helps with connecting the sqlite and creating and connecting and performing operation with database \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcd0f7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: c:\\Users\\ADMIN\\Desktop\\data analytics project\\vendor_Sales_Analysis\n"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "print(\"Current Directory:\", os.getcwd())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ae7edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating engine now and a database \n",
    "engine = create_engine('sqlite:///inventory.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed3b381c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin_inventory.csv\n",
      "end_inventory.csv\n",
      "purchases.csv\n",
      "purchase_prices.csv\n",
      "sales.csv\n",
      "vendor_invoice.csv\n"
     ]
    }
   ],
   "source": [
    "base_dir = r'C:\\Users\\ADMIN\\Desktop\\data analytics project'\n",
    "data_dir = os.path.join(base_dir, 'data1')\n",
    "\n",
    "for file in os.listdir(data_dir):\n",
    "    print(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1e0196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have to create a function for ingestion of data cause in companies the works is done by scripting\n",
    "# this script is writing cause in companies we have continous data coming so make it to load in those csv \n",
    "# automatically  to the database we were writing these function on interval break or on a particular time period \n",
    "def ingest_db(df, table_name, engine, chunksize=10000):\n",
    "    '''this function will ingest the database in a dataframe '''\n",
    "    df.to_sql(table_name, con= engine , if_exists= \"replace\" , index= False)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0248a49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(206529, 9)\n",
      "(224489, 9)\n",
      "(2372474, 16)\n",
      "(12261, 9)\n",
      "(12825363, 14)\n",
      "(5543, 10)\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(data_dir, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(df.shape)\n",
    "        \n",
    "        # Assuming ingest_db is already defined\n",
    "        ingest_db(df, file[:-4], engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36b34aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing logs \n",
    "import logging \n",
    "\n",
    "\n",
    "logs_dir = 'logs'\n",
    "\n",
    "# Check if the directory exists, if not, create it\n",
    "if not os.path.exists(logs_dir):\n",
    "    os.makedirs(logs_dir)\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='logs/ingestion_db.logs',\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    filemode='a'\n",
    ")\n",
    "\n",
    "logging.info(\"Logging is set up.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9ae8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' we also add a time there so we can check what long time it take to load it in database \n",
    "start time the end time and the total time to load '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b074fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a logging function\n",
    "'''this function will load csv as dataframe and ingest/insert it into a database  '''\n",
    "def load_raw_data():\n",
    "    start = time.time()\n",
    "    for file in os.listdir(data_dir):\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(data_dir, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            logging.info(f'Ingesting {file} into database')\n",
    "            ingest_db(df, file[:-4], engine)\n",
    "    end_time = time.time()\n",
    "    total_time = (end_time - start) / 60  # corrected: end_time - start\n",
    "    logging.info(\"Ingestion complete\")\n",
    "    logging.info(f'Time taken {total_time:.2f} minutes')\n",
    "    print(\"hello\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85d4950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "from sqlalchemy import create_engine \n",
    "import logging \n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "engine = create_engine('sqlite:///inventory.db')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
